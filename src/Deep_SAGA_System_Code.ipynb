{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_SAGA_System_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyHP1uCR4YhhcQI5D+urRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliDeane/Deep_SAGA/blob/master/Deep_SAGA_System_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw1l_SBF_M9y"
      },
      "source": [
        "# Deep SAGA\n",
        "\n",
        "To run this system, please ensure that you have a folder in Google Drive (e.g., entitled 'Gaze_Detection_System') containing the following files:\n",
        "\n",
        "\n",
        "\n",
        "*   your_video_file.mp4\n",
        "*   your_gaze_coordinates.csv\n",
        "*   maskrcnn_predict.py\n",
        "*   coco_labels.txt\n",
        "*   mask_rcnn_coco.h5\n",
        "\n",
        "NB. The above files (including example video) can be found at: https://drive.google.com/drive/folders/1OAYbOeE5eIWzM-iwA0Agcg3M9MteN8JQ?usp=sharing \n",
        "\n",
        "\n",
        "Before running, change the runtime type to GPU (Runtime > Change Runtime Type > GPU). Next, insert required variables into the code:\n",
        "\n",
        "* the path to your Google Drive folder, \n",
        "* the name of your video and gaze data files\n",
        "\n",
        "Then run all cells. Once cell will prompt you to mount your Google Drive. When this occurs, follow the link and paste the provided URL into the insert box. \n",
        "\n",
        "\n",
        "\n",
        "Once all cells have finished running, the system will produce the following output files:\n",
        "\n",
        "*   A video labelled with the currently gazed upon object for each frame\n",
        "*   A list of all gazed upon objects for each frame of footage\n",
        "*   Summary statistics outlining the average time spent gazing at people, vehicles and greenery.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu7kj6U0DovP"
      },
      "source": [
        "\"\"\" Insert the filenames and path to the Google drive folder where all files are kept \"\"\"\n",
        "\n",
        "path = r\"/content/gdrive/My Drive/Gaze_Detection_System\" # Insert the path to your Google Drive folder\n",
        "gaze_filename = \"GD_pp1_s1_City_vidA.csv\" # Insert your video filename here\n",
        "video_filename = \"pp1_s1_City_vidA.mp4\" # Insert your gaze data filename here"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJNvuKcGX_pG"
      },
      "source": [
        "\"\"\" Set the greenery values. These define which pixels will be identified as green. It is dependant on the lighting conditions during filming.\n",
        "Currently set to default, but this can be changed. \"\"\"\n",
        "\n",
        "greenery_value_A = 80\n",
        "greenery_value_B = 90"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f7upt1O_39p",
        "cellView": "form"
      },
      "source": [
        "#@title Import all necessary packages. \n",
        "#@markdown You may have to restart the runtime and run this again to ensure that the correct versions of packages are installed.\n",
        "# Import all the main packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import colorsys\n",
        "import argparse\n",
        "import imutils\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from datetime import datetime\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "if h5py.__version__ != '2.10.0':\n",
        "  !pip install h5py==2.10.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orO_BN6O__kY",
        "cellView": "form"
      },
      "source": [
        "#@title Mount the Google Drive\n",
        "#@markdown When prompted, follow the link, sign in to your Google Drive account and copy the provided URL in the box below.\n",
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anIaaN0ODLmq",
        "cellView": "form"
      },
      "source": [
        "#@title Load in Tensorflow and the pre-trained Mask RCNN model\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "#Check that tensorflow is running using the GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)\n",
        "\n",
        "# Grab the MRCNN model from github\n",
        "!pip install git+https://github.com/matterport/Mask_RCNN.git\n",
        "import mrcnn\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import model as modellib\n",
        "from mrcnn import visualize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "prDXOv_dFbBn"
      },
      "source": [
        "#@title Load in MASK RCNN files and initiate the model\n",
        "\n",
        "## Load in all the necessary files for the MRCNN and initialise the network  \n",
        "\n",
        "# load the class label names, one label per line\n",
        "os.listdir(path)\n",
        "CLASS_NAMES = open(os.path.join(path,\"coco_labels.txt\")).read().strip().split(\"\\n\")\n",
        "\n",
        "# generate random (but visually distinct) colors for each class label\n",
        "# (thanks to Matterport Mask R-CNN for the method!)\n",
        "hsv = [(i / len(CLASS_NAMES), 1, 1.0) for i in range(len(CLASS_NAMES))]\n",
        "COLORS = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n",
        "random.seed(42)\n",
        "random.shuffle(COLORS)\n",
        "\n",
        "class SimpleConfig(Config):\n",
        "\t# give the configuration a recognizable name\n",
        "\tNAME = \"coco_inference\"\n",
        "\tGPU_COUNT = 1\n",
        "\tIMAGES_PER_GPU = 1\n",
        "\tNUM_CLASSES = len(CLASS_NAMES)\n",
        "  \n",
        "  # initialize the inference configuration\n",
        "config = SimpleConfig()\n",
        "\n",
        "weights_path =os.path.join(path, \"mask_rcnn_coco.h5\") \n",
        "print(\"[INFO] loading Mask R-CNN model...\")\n",
        "model = modellib.MaskRCNN(mode=\"inference\", config=config,\n",
        "model_dir=os.getcwd())\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "\n",
        "# Add our own class_names to the class names list\n",
        "CLASS_NAMES.append('Background')\n",
        "CLASS_NAMES.append('Greenery')\n",
        "CLASS_NAMES.append('OOB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "VWA8Ctp9V00e"
      },
      "source": [
        "#@title Load in video recording and Gaze coordinates\n",
        "#@markdown This loads in the video and gaze coordinates. It checks that the eye tracker did not miss too many frames when capturing gaze location.\n",
        "\n",
        "\n",
        "def load_gaze_data(path, gaze_filename):\n",
        "\n",
        "  \"\"\" Loads in the gaze data. Returns list for X and Y coordinates \"\"\"\n",
        "\n",
        "  raw_gaze = open(os.path.join(path, gaze_filename)).read().strip().split(\"\\n\")\n",
        "\n",
        "  raw_gaze.pop(0) # remove the x and y\n",
        "  GX = [round(float(pair.split(\",\")[0])) for pair in raw_gaze]\n",
        "  GY = [round(float(pair.split(\",\")[1])) for pair in raw_gaze]\n",
        "  return GX, GY\n",
        "\n",
        "def load_video(path, video_filename):\n",
        "  \"\"\" Loads in the video and returns a frame-by-frame video variable vs. Also returns video output path\"\"\"\n",
        "  vid_input = os.path.join(path, video_filename)\n",
        "  vid_output = os.path.join(path, \"output_\" + video_filename)\n",
        "\n",
        "  #Initialise the video stream and pointer to output video file\n",
        "  vs = cv2.VideoCapture(vid_input)\n",
        "  writer = None\n",
        "\n",
        "  return vs, vid_output\n",
        "\n",
        "def check_frames(vs):\n",
        "  \"\"\" Check that the number of frames in which the eye tracker failed to collect any data isn't too high\"\"\"\n",
        "\n",
        "  # Count number of frames in video\n",
        "  prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT if imutils.is_cv2() else cv2.CAP_PROP_FRAME_COUNT\n",
        "  total = int(vs.get(prop))\n",
        "  print(\"[INFO] {} total frames in video\".format(total))\n",
        "\n",
        "  # compare to number of gaze datapoints to check the gaze tracker didn't miss to many\n",
        "  dropped_frames = total - len(GX)\n",
        "\n",
        "  if dropped_frames < 10:\n",
        "    [GX.append(0) for i in range(0,dropped_frames)]\n",
        "    [GY.append(0) for i in range(0,dropped_frames)]\n",
        "    print('[INFO] The eye tracker dropped {} frames. {} 0s have been added to each gaze coordinate list.'.format(dropped_frames, dropped_frames))\n",
        "\n",
        "  elif dropped_frames > 10:\n",
        "    raise Exception('The number of dropped frames is too high: {}'.format(dropped_frames))\n",
        "  \n",
        "\n",
        "\"\"\" Load in the video and gaze data\"\"\"\n",
        "GX, GY = load_gaze_data(path, gaze_filename)\n",
        "video, vid_output = load_video(path,video_filename)\n",
        "check_frames(vs = video)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oJNrMICiZqiy"
      },
      "source": [
        "#@title SAGA For Loop Functions\n",
        "\"\"\" SAGA For Loop Functions \"\"\"\n",
        "\n",
        "def draw_mrcnn_output(frame, startX, startY, endX, endY, color, label, score):\n",
        "  cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "  text = \"{}: {:.3f}\".format(label, score)\n",
        "  y = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "  cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "    0.6, color, 2)   \n",
        "\n",
        "def apply_green_overlay(frame, gmask, greenery_score):\n",
        "    overlay = frame.copy()\n",
        "    grindex = np.where(gmask == 255) # Find the pixels that are green\n",
        "    overlay[grindex[0][:],grindex[1][:],0] = 250 # Change these pixels in the overlay - make blue value high\n",
        "    overlay[grindex[0][:],grindex[1][:],1] = 20 # Green value low\n",
        "    overlay[grindex[0][:],grindex[1][:],2] = 20 # Red value low\n",
        "    alpha = 0.6\n",
        "    cv2.addWeighted(overlay, alpha, frame, 1-alpha, 0 , frame)\n",
        "\n",
        "    if len(grindex[0][:]):\n",
        "      temp_greenery_score = (len(grindex[0][:]) / (720*1280)) * 100\n",
        "      greenery_score.append(temp_greenery_score) #  This stores the percentage of the given frame that was identified as green. \n",
        "\n",
        "    return greenery_score\n",
        "\n",
        " \n",
        "def identify_inframe_objects(GY, GX, count, mask, inframe_gaze_checklist, confidence_list, inframe_object_loc_X,\\\n",
        "                          inframe_object_loc_Y, startX, startY):\n",
        " \n",
        "\n",
        "  if GY[count] > 0 and GY[count] < 720 and GX[count] > 0 and GX[count] < 1280: # If the gaze fell within the headview camera's boundaries\n",
        "    if mask[GY[count],GX[count]]: # == True:\n",
        "      #label_winner = classID\n",
        "      inframe_gaze_checklist.append(CLASS_NAMES[classID]) # inframe_gaze_checklist is the objects appearing in the given frame\n",
        "      confidence_list.append(2)\n",
        "      inframe_object_loc_X.append(startX)\n",
        "      inframe_object_loc_Y.append(startY)\n",
        "\n",
        "    else:\n",
        "      #label_winner = 'Background'\n",
        "      inframe_gaze_checklist.append(0)\n",
        "      confidence_list.append(0)\n",
        "      inframe_object_loc_X.append(0)\n",
        "      inframe_object_loc_Y.append(0)\n",
        "      \n",
        "  else: # If gaze did not fall within the head view camera boundaries then add 'outofbounds' to the gazed_upon_object_list list\n",
        "    label_winner = 'Out Of Bounds'\n",
        "    inframe_gaze_checklist.append('OOB')\n",
        "    confidence_list.append(0)\n",
        "    inframe_object_loc_X.append(0)\n",
        "    inframe_object_loc_Y.append(0)\n",
        "\n",
        "  return inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y\n",
        "\n",
        "\n",
        "def get_gazed_upon_object(inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y, gazed_upon_object_list, confidence, gmask):\n",
        "\n",
        "  gazed_upon_index = [i for i, e in enumerate(inframe_gaze_checklist) if e != 0] # gazed_upon_index is the index of the winning classID (if there is one)\n",
        "\n",
        "\n",
        "  if len(gazed_upon_index) > 0: # If there is a single winning object then add the winner to the gazed_upon_object_list list    \n",
        "    new_champ = inframe_gaze_checklist[gazed_upon_index[0]]  \n",
        "    current_startX = inframe_object_loc_X[gazed_upon_index[0]] # This defines the current location of the gazed upon object\n",
        "    current_startY = inframe_object_loc_Y[gazed_upon_index[0]]\n",
        "    \n",
        "    if new_champ == 59: # if it's a potted plant, then mark as green\n",
        "      gazed_upon_object_list.append('Greenery')\n",
        "    else: # If the recognised object is not a potted plant, then add that classID to the gazed_upon_object_list list\n",
        "      gazed_upon_object_list.append(new_champ)\n",
        "\n",
        "  elif len(gazed_upon_index) == 0: # If no winning object was found, then check if green is being looked at\n",
        "    \n",
        "    if GY[count] > 0 and GY[count] < 720 and GX[count] > 0 and GX[count] < 1280: # If the gaze fell within the headview camera's boundaries \n",
        "      if gmask[GY[count],GX[count]] == 255: # If the gaze coords falls on a green area (The mask is flipped - so is GY,GX)\n",
        "        new_champ = 'Greenery'\n",
        "        gazed_upon_object_list.append('Greenery') # 82 is greenery\n",
        "        confidence.append(0)\n",
        "        \n",
        "      elif gmask[GY[count],GX[count]] == 0:\n",
        "        new_champ = 'Background'\n",
        "        gazed_upon_object_list.append('Background')\n",
        "        confidence.append(0)\n",
        "\n",
        "    else:\n",
        "      new_champ = 'OOB'\n",
        "\n",
        "  return gazed_upon_object_list, confidence, new_champ\n",
        "\n",
        "\n",
        "def overlay_label(CLASS_NAMES, new_champ, frame):\n",
        "  # Draw the Text on top\n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "  org = (50, 50)       \n",
        "  fontScale = 2      \n",
        "  color = (0, 0, 255)  # Red     \n",
        "  thickness = 2      \n",
        "  # winner_label = CLASS_NAMES[new_champ] # text to draw\n",
        "  image = cv2.putText(frame, new_champ, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA) \n",
        "  return image\n",
        "\n",
        "\n",
        "def overlay_gaze_cursor(GX, GY, count, frame):\n",
        "  center_coordinates = (GX[count], GY[count]) \n",
        "  radius = 30 \n",
        "  color = (0, 0, 255) \n",
        "  thickness = -1\n",
        "    \n",
        "  # Using cv2.circle() method \n",
        "  # Draw a circle of red color of thickness -1 px \n",
        "  image = cv2.circle(frame, center_coordinates, radius, color, thickness) \n",
        "  return image\n",
        "\n",
        "def print_user_info(count, total):\n",
        "  if count == 8:\n",
        "    elap = (end - start)\n",
        "    print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
        "    print(\"[INFO] estimated total time to finish: {:.4f}\".format((elap * (total/4))))    \n",
        "  elif count == round(total/8): #round(quartal/2):\n",
        "    print(\"[INFO] Halfway!\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "f3kDQllpcaK5"
      },
      "source": [
        "#@title Main Gaze Detection For Loop\n",
        "# Define the lists that will be built in the following while loop\n",
        "gazed_upon_object_list = [] # List of objects that were being looked at for each frame\n",
        "confidence = [] # List saying whether the gaze is in the mask/just in the box\n",
        "green_list = []\n",
        "greenery_score = []\n",
        "persons_in_video = []\n",
        "vehicles_in_video = []\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Begin the processing for loop. This finds the coordinates and labels for each object and computes a greenery mask. \n",
        "It compares the gaze coordinates with the object coordinates/greenery mask to determine what is being looked at for each frame.\n",
        "The final frame-by-frame list is stored in the 'gazed_upon_object_list' and the 'average_persons_per_frame' and \n",
        "'average_vehicles_per_frame' are an show the average number of persons/vehicles in each frame. The average_greenery_score\n",
        "is the average percentage of pixels in each frame that were tagged as green.\n",
        "\"\"\"\n",
        "now = datetime.now() #Print the current time for timing checks\n",
        "current_time = now.strftime(\"%H:%M:%S\")\n",
        "print('[INFO] Starting Time =', current_time)\n",
        "\n",
        "for count in range(0, total):\n",
        "  \n",
        "  start = time.time()\n",
        "  # read the next frame from the file\n",
        "  (grabbed, frame) = vs.read()\n",
        "  \n",
        "  if (count % 4) == 0: # We only process 1 in 4 frames (to save time). Delete this if statement to process all frames\n",
        "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
        "    gmask = cv2.inRange(hsv,(30, greenery_value_A, 20), (greenery_value_B, 255, 255) )\n",
        "    r = model.detect([frame], verbose=0)[0] # verbose tells us whether we want to see the output or not\n",
        "\n",
        "    inframe_gaze_checklist = []\n",
        "    inframe_labels = []\n",
        "    confidence_list = []\n",
        "    inframe_object_loc_X = []\n",
        "    inframe_object_loc_Y = []\n",
        "    persons_in_frame = 0\n",
        "    vehicles_in_frame = 0\n",
        "\n",
        "    for object in range(0, r[\"rois\"].shape[0]): # Loop over class labels\n",
        "      # extract the class ID and mask for the current detection, then\n",
        "      # grab the color to visualize the mask (in BGR format)\n",
        "      classID = r[\"class_ids\"][object]\n",
        "      mask = r[\"masks\"][:, :, object]\n",
        "      color = COLORS[classID][::-1]\n",
        "\n",
        "      # visualize the pixel-wise mask of the object\n",
        "      frame = visualize.apply_mask(frame, mask, color, alpha=0.5)\n",
        "\n",
        "      # extract the bounding box information, class ID, label, predicted\n",
        "      # probability, and visualization color\n",
        "      (startY, startX, endY, endX) = r[\"rois\"][object]\n",
        "      classID = r[\"class_ids\"][object]\n",
        "      label = CLASS_NAMES[classID]\n",
        "      score = r[\"scores\"][object]\n",
        "      color = [int(c) for c in np.array(COLORS[classID]) * 255]\n",
        "      inframe_labels.append(classID)\n",
        "\n",
        "      if classID == 1: # If it's a person, add one to the person count list\n",
        "        persons_in_frame += 1\n",
        "      elif classID == 3 or classID == 4 or classID == 6 or classID == 8: # If object is a car, bus, motorbike or truck, then add that to the list\n",
        "        vehicles_in_frame +=1\n",
        "\n",
        "      draw_mrcnn_output(frame, startX, startY, endX, endY, color, label, score)\n",
        "\n",
        "      # Find labels and coords for all objects in a frame\n",
        "      inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y = identify_inframe_objects(GY, GX, count, mask, inframe_gaze_checklist, confidence_list, inframe_object_loc_X,\\\n",
        "                          inframe_object_loc_Y, startX, startY)\n",
        "\n",
        "      # Now combine the frames together to produce the output video     \n",
        "      if writer is None:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") # Initialises the video writer\n",
        "        writer = cv2.VideoWriter(vid_output, fourcc, 10,\n",
        "          (frame.shape[1], frame.shape[0]), True) # Vid output is the path to the drive folder where the output video will go  \n",
        "    \n",
        "    greenery_score = apply_green_overlay(frame, gmask, greenery_score) # Find percentage of frame that is green greenery_score and apply greenery mask to the frame\n",
        "    persons_in_video.append(persons_in_frame) # Add the v and p counts so that index 1 gives the number of people/vehicles in the first frame, same for index 2 etc....\n",
        "    vehicles_in_video.append(vehicles_in_frame)\n",
        "    \n",
        "    \n",
        "    gazed_upon_index = [i for i, e in enumerate(inframe_gaze_checklist) if e != 0] # gazed_upon_index is the index of the winning classID (if there is one)\n",
        "\n",
        "    gazed_upon_object_list, confidence, new_champ = get_gazed_upon_object(inframe_gaze_checklist, confidence_list, inframe_object_loc_X, inframe_object_loc_Y, gazed_upon_object_list, confidence, gmask)\n",
        "\n",
        "    overlay_label(CLASS_NAMES, new_champ, frame) # Change to not being == to image\n",
        "    overlay_gaze_cursor(GX, GY, count, frame)\n",
        "\n",
        "    end = time.time()\n",
        "    writer.write(frame)\n",
        "\n",
        "  print_user_info(count, total)\n",
        "\n",
        "# release the file pointers\n",
        "print(\"[INFO] cleaning up...\")\n",
        "writer.release()\n",
        "vs.release()\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "e5wh9Me6clbh"
      },
      "source": [
        "#@title Download the gaze statistics files\n",
        "#@markdown This downloads two files: one contains gaze statistics, the other is a list of objects gazed upon for each frame of footage.\n",
        "\n",
        "def get_gaze_stats(persons_in_video, vehicles_in_video, greenery_score, total, video_filename):\n",
        "  \"\"\" Generates and downloads a file of summary statistics  \"\"\"\n",
        "\n",
        "  average_persons_per_frame = (sum(persons_in_video)/len(persons_in_video))\n",
        "  average_vehicles_per_frame = (sum(vehicles_in_video)/len(vehicles_in_video))\n",
        "  average_greenery_score = (sum(greenery_score)/len(greenery_score))\n",
        "  key_features_scores_list = [int(round(total/4)), average_persons_per_frame, average_vehicles_per_frame, average_greenery_score]\n",
        "\n",
        "  key_feature_scores_df = pd.DataFrame(columns=['Feature', 'Score'])\n",
        "  key_feature_scores_df['Feature'] = ['Num of processed frames','Persons per frame', 'Vehicles per frame', 'Greenery per frame (% of pixels)']\n",
        "  key_feature_scores_df['Score'] = key_features_scores_list\n",
        "\n",
        "  output_filename = \"summary_scores_\" + video_filename\n",
        "  key_feature_scores_df.to_csv(output_filename, index = False)\n",
        "  files.download(output_filename)\n",
        "\n",
        "\n",
        "def get_GazedUpon_file(gazed_upon_object_list, video_filename):\n",
        "\n",
        "  \"\"\" Returns csv file of objects gazed upon for each frame \"\"\"\n",
        "\n",
        "  output_filename = \"gazed_upon_objects\" + video_filename\n",
        "  GU_list_df = pd.DataFrame(gazed_upon_object_list) \n",
        "  GU_list_df.to_csv(output_filename, index = False)\n",
        "  files.download(output_filename)\n",
        "\n",
        "get_gaze_stats(persons_in_video, vehicles_in_video, greenery_score, total, video_filename)\n",
        "get_GazedUpon_file(gazed_upon_object_list, video_filename)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9GQiPS93kro-"
      },
      "source": [
        "#@title Functions for Generating Graphs\n",
        "def normalise_data(data):\n",
        "  data_set_edit = []\n",
        "  for i in range(0,len(data)):\n",
        "    data_set_edit.append(data[i])# [0])\n",
        "  return data_set_edit\n",
        "\n",
        "def prepare_for_graphing(data_set, unique_cats): # Prepare data for graphing\n",
        "  unique_cats_2 = unique_cats\n",
        "  data_set_2 = data_set\n",
        "  graph_list = []\n",
        "  for category in unique_cats:\n",
        "    graph_list.append([i for i, e in enumerate(data_set) if e == category])\n",
        "\n",
        "\n",
        "  GU_object_graphing_dict = {}\n",
        "  for i in range(0,len(unique_cats)):\n",
        "    GU_object_graphing_dict.update( {unique_cats[i]: get_broken_bar_list(graph_list[i])} )\n",
        "    \n",
        "  return GU_object_graphing_dict\n",
        "\n",
        "# Function to get final list for the broken bar chart\n",
        "def get_broken_bar_list(gaze_object_list):\n",
        "  output_list = []\n",
        "  for i in gaze_object_list:\n",
        "    output_list.append((i,1))\n",
        "\n",
        "  return output_list\n",
        "\n",
        "def generate_graph(GU_object_graphing_dict, unique_cats, system_data, ax):\n",
        "\n",
        "  y_tick_labels = [] # For the Ytick Labels\n",
        "  iterations = []\n",
        "\n",
        "  color_list = ['purple', 'olive', 'brown', 'cyan', 'pink']*15\n",
        "  color_dict = {}\n",
        "  count = 0\n",
        "  # Get colour for the graphing\n",
        "  for i in unique_cats:\n",
        "    if i == 'Greenery':\n",
        "      color_dict.update( {i: 'green'} )\n",
        "    elif i == 'Person':\n",
        "      color_dict.update( {i:'red'})\n",
        "    elif i == 'Vehicle':\n",
        "      color_dict.update( {i:'orange'})\n",
        "    elif i == 'Background':\n",
        "      color_dict.update( {i:'blue'})\n",
        "    else:\n",
        "      color_dict.update( {i: color_list[count]} )\n",
        "    count += 1\n",
        "\n",
        "\n",
        "  count = 0\n",
        "  for i in GU_object_graphing_dict:\n",
        "    count += 1\n",
        "    ax.broken_barh(GU_object_graphing_dict[i], ((count*5), 5), facecolors='tab:{}'.format(color_dict[i])) \n",
        "\n",
        "    y_tick_labels.append(count*5+2.5)\n",
        "    iterations.append(count*5)\n",
        "\n",
        "\n",
        "  ax.set_ylim(5, len(unique_cats)*5+5)\n",
        "  ax.set_xlim(0, len(system_data))\n",
        "  ax.set_xlabel('Frame Number')\n",
        "  ax.set_ylabel('Gazed Upon Object')\n",
        "\n",
        "  # if title:\n",
        "  #   ax.set_title('Objects Gazed At For Each Frame Of Footage As Generated By The Novel System and The Human Coder'.format(coder_identity))\n",
        "\n",
        "  ax.set_yticks(y_tick_labels)\n",
        "  ax.set_yticklabels(unique_cats)\n",
        "  ax.grid(False)\n",
        "\n",
        "  iterations = [10,15,20,25,30] # Add in lines around bars\n",
        "  for i in iterations:\n",
        "    ax.axhline(y=i,linewidth=1, color='gray', alpha = 0.3)\n",
        "\n",
        "  return ax"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eLaPH4ALesUk"
      },
      "source": [
        "#@title Generate and Download Summary Visualisations\n",
        "normal_system_data = normalise_data(gazed_upon_object_list)\n",
        "system_unique_categories = list(set(normal_system_data)) # Lists all unique objects appearing in GU list\n",
        "GU_object_graphing_dict = prepare_for_graphing(normal_system_data, system_unique_categories)\n",
        "\n",
        "#Initiate Graph\n",
        "fig, plotter = plt.subplots(1, figsize=(8,3), sharex = True, gridspec_kw = {'hspace' : 0.05})\n",
        "plt.rc('font', family='sans serif')\n",
        "plt.rc('xtick', labelsize='x-small')\n",
        "plt.rc('ytick', labelsize='x-small')\n",
        "\n",
        "ax = generate_graph(GU_object_graphing_dict, system_unique_categories, gazed_upon_object_list, plotter)\n",
        "\n",
        "fig.savefig('Comparison_Fig_Video_{}.svg'.format(date), format='svg', dpi=1200) # , dpi=1200)\n",
        "files.download('Comparison_Fig_Video_{}.svg'.format(date))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6XXq0eue0EP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
